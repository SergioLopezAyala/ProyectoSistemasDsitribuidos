# Sistema Distribuido de AsignaciÃ³n de Recursos AcadÃ©micos

Este proyecto implementa un sistema distribuido para asignar **salones**, **laboratorios** y **aulas mÃ³viles** a diferentes programas acadÃ©micos agrupados por facultades. Utiliza comunicaciÃ³n basada en **ZeroMQ**, tolerancia a fallos mediante un **servidor rÃ©plica**, y captura de **mÃ©tricas de rendimiento** para anÃ¡lisis y validaciÃ³n del sistema.

---

## ğŸ“¦ Componentes del Sistema

- `ServidorPrincipal.py`: Atiende solicitudes de facultades y realiza las asignaciones.
- `ServidorReplica.py`: Servidor de respaldo activado automÃ¡ticamente si falla el principal.
- `HealthCheck.py`: Supervisa el servidor principal y activa el rÃ©plica si no responde.
- `Facultad.py`: Agrupa solicitudes de programas y las reenvÃ­a al servidor.
- `Programa.py`: Envia su solicitud de recursos a su facultad correspondiente.
- `Lanzador.py`: Orquestador que lanza mÃºltiples facultades y programas desde una sola mÃ¡quina.

---

## âš™ï¸ Requisitos

- Python 3.8 o superior.
- Dependencias:
  - `pyzmq`
  - `psutil`

InstalaciÃ³n de dependencias:

```bash
pip install pyzmq psutil
ğŸ–¥ï¸ Despliegue en 3 MÃ¡quinas
ğŸŸ¢ MÃ¡quina 1: Servidor Principal
IP sugerida: 10.43.103.220

Inicia el servidor:

bash
Copiar
Editar
python3 ServidorPrincipal.py
Este servidor se queda escuchando solicitudes entrantes de las facultades a travÃ©s del puerto 5555.

ğŸ” MÃ¡quina 2: Health Checker + Servidor RÃ©plica
IP sugerida del servidor rÃ©plica: 10.43.96.9

Inicia el script de monitoreo:

bash
Copiar
Editar
python3 HealthCheck.py
Este script intenta hacer ping al servidor principal periÃ³dicamente. Si detecta 3 fallos consecutivos, lanza el ServidorReplica.py en el mismo host.

ğŸš€ MÃ¡quina 3: Lanzador de Facultades y Programas
Inicia el lanzador con el nÃºmero de facultades y los recursos a solicitar:

bash
Copiar
Editar
python3 Lanzador.py <num_facultades> <salones_por_programa> <laboratorios_por_programa>
Ejemplo:

bash
Copiar
Editar
python3 Lanzador.py 5 6 3
Esto lanzarÃ¡:

5 facultades (Facultad_1 a Facultad_5), cada una escuchando en puertos consecutivos desde el 7000.

Cada facultad genera 5 programas que se comunican por su puerto correspondiente.

Cada programa solicita 6 salones y 3 laboratorios.

ğŸ“Š MÃ©tricas y Logs
Cada componente genera archivos de log para seguimiento y anÃ¡lisis de rendimiento:

Componente	MÃ©tricas Capturadas	Archivo generado
Facultad.py	Latencia, CPU, RAM, resultado por programa	log_<nombre_facultad>.txt
HealthCheck.py	Tiempo de ping, fallos, activaciÃ³n de rÃ©plica	log_healthcheck.txt
ServidorPrincipal.py	Tiempo de respuesta, CPU, RAM, asignaciones	log_metricas_servidor.txt + log_asignaciones.json
ServidorReplica.py	MÃ©tricas similares a principal	log_metricas_replica.txt + log_asignaciones_replica.json
Lanzador.py	PID, tiempos de ejecuciÃ³n de procesos	log_superlanzador.txt

ğŸ“ Estructura del Proyecto
pgsql
Copiar
Editar
.
â”œâ”€â”€ Facultad.py
â”œâ”€â”€ Programa.py
â”œâ”€â”€ ServidorPrincipal.py
â”œâ”€â”€ ServidorReplica.py
â”œâ”€â”€ HealthCheck.py
â”œâ”€â”€ Lanzador.py
â”œâ”€â”€ log_asignaciones.json
â”œâ”€â”€ log_asignaciones_replica.json
â”œâ”€â”€ log_metricas_servidor.txt
â”œâ”€â”€ log_metricas_replica.txt
â”œâ”€â”€ log_healthcheck.txt
â”œâ”€â”€ log_superlanzador.txt
â”œâ”€â”€ log_<nombre_facultad>.txt
â””â”€â”€ README.md
ğŸ§ª EjecuciÃ³n de Pruebas
El sistema ha sido probado con 5, 7 y hasta 10 facultades.

Cada facultad puede gestionar mÃºltiples programas que solicitan diferentes cantidades de recursos.

La tolerancia a fallos ha sido validada mediante la desconexiÃ³n manual del servidor principal.

ğŸ“Œ Consideraciones
AsegÃºrate de que las mÃ¡quinas involucradas pueden comunicarse por los puertos usados (5555, 5556, 7000+).

Todos los scripts deben estar sincronizados en todas las mÃ¡quinas involucradas.

Puedes agregar seguridad adicional mediante autenticaciÃ³n o validaciÃ³n IP si lo deseas.

ğŸ“ˆ Extensiones recomendadas
Analizador automÃ¡tico de logs para generar CSV o grÃ¡ficas.

Dashboard en tiempo real para monitorear mÃ©tricas.

AutenticaciÃ³n entre nodos (opcional).

Desarrollado como parte del Proyecto Final â€” Semestre 2025-10
